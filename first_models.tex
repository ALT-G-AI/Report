\section{First Models}
\label{sec:first_models}

\subsection{Bag of Words}
\label{sec:bag_of_words}
From the examination of the data in Section \ref{sec:word_frequency}, a bag-of-words classifier seems plausible. Through a direct comparison of the frequencies with which each author uses the words in a given sentence, a prediction may be formed.

Let $\phi_w^a$ be the number of occurrences of some word $w$ in the corpus of sentences attributed to author $a$. The estimated probability of an author's usage of a word, $w^*$, is given by:

\begin{equation*}
p_{w*}^a = \frac{\phi_w^a}{\sum\limits_{w} \phi_w^a}
\end{equation*}

The estimated probability of a sentence, $s$, is given by:

\begin{align*}
p_{s}^a &= \prod\limits_{w\in s} p_{w}^a \\
log(p_{s}^a &)= \sum\limits_{w\in s} log(p_{w}^a)
\end{align*}

During prediction, however, it is likely that certain words will not have occurred in the training data: a "miss". An intuitive way to handle this is to score authors first on the number of words they "hit", and then by the probability of those words. Denoting the words used by each author in the training set as $W_a$, the following scoring mechanism is introduced:

\begin{align*}
score(s,a) &= \sum\limits_{w\in s}
\begin{cases}
log(p_{w}^a) & w\in W_a \\
0 & otherwise
\end{cases}\\
hits(s,a) &= \sum\limits_{w\in s}
\begin{cases}
1 & w\in W_a \\
0 & otherwise
\end{cases}\\
\end{align*}

Authors are scored first by $hits$, and then differentiated based on $score$

Running this model on the training set with 3 cross-validation passes (such that each third of the training data is predicted based on a model trained on the remaining two-thirds) yields the confusion matrix and accuracy in Table \ref{tab:bow_cw}. Log-loss metrics are not available for this model as it doesn't produce output probabilities, only binary classifications.

\begin{table}[h]
\centering
\begin{tabular}{m{1cm}|m{1cm}|m{1cm}|m{1cm}|m{0cm}}
\multicolumn{1}{m{1cm}}{} & \multicolumn{1}{m{1cm}}{EAP} & \multicolumn{1}{m{1cm}}{HPL} & \multicolumn{1}{m{1cm}}{MWS} &\\[5pt]
\cline{2-4}
EAP & 4910 & 630 & 760 & \\[5pt]
\cline{2-4}
HPL & 476 & 3774 & 283 & \\[5pt]
\cline{2-4}
MWS & 615 & 381 & 3834 & \\[5pt]
\cline{2-4}
\end{tabular}
\caption{Results for bag-of-words classifier, stemming and lemmatisation enabled.\\Accuracy: 80\%}
\label{tab:bow_cw}
\end{table}

\subsection{Random Forest}
\label{sec:forest}

Based on impressive success in software author recognition using random forest
methods \cite{c3}, a classifier of this type was pursued. Unfortunately the random forest will not be directly suitable for entry into
the log-loss judged Kaggle competition \cite{kaggle} because it outputs only class
predictions and not certainties. Nonetheless, its performance was evaluated for comparison with other models.

Initially the forest was fed encoded sentences directly.

Unfortunately the forest only managed 54\% accuracy. The performance
on the training data was no different than on testing data and so this was not a
generalisation issue - the forest was just not complex enough to learn the
classification rule, resulting in high bias.

The forest suffered a 14\% performance degradation if trained from only windows
instead of full (padded) sentences. It suffered a further 20\% performance
degradation if trained using one-hot encoded labels instead of simply numbering
the class labels. Forest performance was not sensitive to the forest size beyond
200 classifiers. 

\subsection{Linear SVM}
\label{sec:svm}

As almost all code could be shared with the random forest, an SVM was also
tested. A linear kernel was used because the padded input sentence was already a
2500-dimensional space and so was likely linearly separable.

The Linear SVM achieved 52\% accuracy, with identical precision and recall
scores (indicating no bias to one class over another).

\subsection{Windowed Neural Network}
\label{sec:win_nn}
In order to recognise tell-tale structures in the way sentences are composed, a classifier must be able to examine words in context. By running a window over the sentence tokens, this can be achieved.

Using a word embedding of $d$ dimensions, a window of $n$ words is provided as the input to a neural network. These words are flattened into a $d \times n$ wide layer, and fed through some number of hidden Relu units before being fed to a classifying output layer.

The word embedding for this network may either be built on the training set itself or imported from a pre-trained model. The former may create a more subject-specific embedding since it is trained on relevant data; however the relatively small corpus size means that the spacial relationship between words may be weaker and the likelihood of missing words in the test data may be higher.

For this classifier, these embeddings are as described in Section \ref{sec:word2vec}.

In either case, it must be decided how to deal with words in the test data that do not exist in the word embedding. For the embedding built from the training set, unique words (that is, words that appear only once across all sentences) are replaced with a \textit{RARE} token. This creates an embedding for unique words, and when the model is run any words which do not exist in the embedding may be replaced with this token.

For the pre-trained model, inserting tokens is not possible. Instead words that do not exist within the embedding are replaced with a mean vector, $\vec{e}_{rare}$, derived from the embedding $E$, such that:

\begin{equation*}
\vec{e}_{rare} = \frac{1}{\lVert E \rVert} \sum\limits_{\vec{w} \in E}^{\lVert E \rVert} \vec{w}
\end{equation*}


Additionally test sentences with length less than the window size must be padded to at least that length; In this model, such sentences are being appended with the last word in the sentence until long enough.

The confusion matrices, log-losses and accuracies for this model under both encoder methods are shown in Table \ref{tab:window_res}.

\begin{table}[h]
\centering
\begin{subtable}{\columnwidth}
\centering
\begin{tabular}{m{1cm}|m{1cm}|m{1cm}|m{1cm}|m{0cm}}
\multicolumn{1}{m{1cm}}{} & \multicolumn{1}{m{1cm}}{EAP} & \multicolumn{1}{m{1cm}}{HPL} & \multicolumn{1}{m{1cm}}{MWS} &\\[5pt]
\cline{2-4}
EAP & 4504 & 773 & 1023 & \\[5pt]
\cline{2-4}
HPL & 1201 & 2632 & 700 & \\[5pt]
\cline{2-4}
MWS & 1206 & 543 & 3081 & \\[5pt]
\cline{2-4}
\end{tabular}
\caption{Encoding built from training data, stemming and lemmatisation enabled.\\Loss: 1.83 Accuracy: 65\%}
\end{subtable}\\
\vspace{1cm}
\begin{subtable}{\columnwidth}
\centering
\begin{tabular}{m{1cm}|m{1cm}|m{1cm}|m{1cm}|m{0cm}}
\multicolumn{1}{m{1cm}}{} & \multicolumn{1}{m{1cm}}{EAP} & \multicolumn{1}{m{1cm}}{HPL} & \multicolumn{1}{m{1cm}}{MWS} &\\[5pt]
\cline{2-4}
EAP & 4587 & 757 & 956 & \\[5pt]
\cline{2-4}
HPL & 1084 & 2885 & 564 & \\[5pt]
\cline{2-4}
MWS & 1019 & 525 & 3286 & \\[5pt]
\cline{2-4}
\end{tabular}
\caption{Pre-trained GloVe encoder, stemming and lemmatisation enabled.\\Loss: 1.90 Accuracy: 69\%}
\end{subtable}
\caption{Results for windowed neural net classifier. Single hidden layer with 100 neurons, window width of 5. 150 training epochs.}
\label{tab:window_res}
\end{table}

\label{sec:dnn}

\subsection{RNN}
\label{sec:rnn}
  An obvious choice for textual input data was a recursive neural network.
  Three recursive cell types -- basic, LSTM and GRU -- were investigated.  
  
  Figure \ref{fig:rnn-structure} shows the structure of the RNN. As recommended
  by one of the PhD student project advisors, outputs were generated from a
  dense network fed by the internal state of the recursive cell. This dense
  network used a sigmoid activation function so that its outputs ranged between
  0 and 1 and so could learn confidences for each author. 
  
  \begin{figure}[ht]
    \includegraphics[width=0.45\textwidth]{Figures/rnn.png}
    \caption{Recursive neural network structure}
    \label{fig:rnn-structure}
  \end{figure}
  
    The RNN was trained using cross-entropy loss and an Adam optimizer. Data
    were fed in using randomized batches so that the data could all fit into GPU
    memory and to improve generalisation performance. Initially a batch size of
    100 sentences was used but this led to over-fitting of each batch even for
    relatively small networks. When the batch size was increased to 200 this
    problem lessened significantly as a larger batch provided a much
    more representative sample of the dataset. 
    
    In early testing it was predictably found that the basic RNN cell was out
    performed by LSTM and GRU cells. Less obviously, changing the weights
    initialiser from Xavier (Tensorflow default) to He initialisation harmed
    performance. 
    
    The prediction accuracy on the training dataset was observed to be slightly
    higher than on the test dataset. However, this difference remained
    constant as networks were made larger and trained for longer - not showing
    the characteristic increase in testing error as models overfit training
    data. As a result of this behaviour early stopping was not implemented.
    However the overfitting was combatted by imposing dropout during training.  
    
    Early experimentation also investigated the format of the encoded sentences.
    A very significant (almost 30\%) accuracy improvement was gained by using
    whole padded sentences instead of short windows into sentences. Presumably
    the recursive networks were remembering some features throughout the
    sentence. Stemming and stop word removal were also tested and found to have
    a small detrimental effect. 
    
    By the end of this early phase, the RNN achieved 68\% accuracy, with similar
    precision and recall scores: indicating no bias towards one class over
    another. 
    
   
